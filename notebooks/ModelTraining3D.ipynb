{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAb77yZ9fzMG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.networks import nets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchio as tio\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from modules.Dataset import *\n",
    "from modules import Evaluator3D, models, Trainer3D\n",
    "from modules.Transforms import *\n",
    "from modules.LossFunctions import DC_and_CE_loss, GDiceLossV2, IoULoss\n",
    "from modules.Tensorboard import TensorboardModules\n",
    "from modules.Utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAp5iDVkXVo5"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Hyper-parameters\n",
    "params = {\"total_epochs\": 100,\n",
    "          \"batch_size\": 1,\n",
    "          \"patch_size\": (128, 128, 128),\n",
    "          \"SGD\": {\"lr\": 1e-01, \"momentum\": 0.9, \"nesterov\": True},\n",
    "          #\"Adam\": {\"lr\": 1e-05, \"betas\":(0.9, 0.999), \"eps\": 1e-8},       \n",
    "          \"ES\":{\"patience\": 10, \"min_delta\": 1e-03},\n",
    "          \"CLR\":{\"base\": 1e-05, \"max\": 1e-02, \"up\": 3, \"down\": 5, \"mode\": \"triangular2\"},\n",
    "          #\"SLR\":{'step_size': 10, \"gamma\": 1e-1}  ,\n",
    "          #\"CALR\": {'T_max': 100, 'eta_min': 1e-05}\n",
    "         }\n",
    "\n",
    "output_path = ''.join(['output/UNet3D/Iteration_20230107/SDUNet_4'])#, datetime.now().strftime(\"%Y%m%d_%H%M\")])\n",
    "weight_path = os.path.join(output_path, \"weights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VWdtdClXW53"
   },
   "outputs": [],
   "source": [
    "# There are multiple data folders belong to same dataset. Each of them processed in different ways.\n",
    "# Therefore, path of the data and its name explicitly are defined.\n",
    "dataset_train = FeTABalancedDistribution\n",
    "dataset_path_train = os.path.join('Data', 'lucas')\n",
    "\n",
    "dataset_val = FeTABalancedDistribution\n",
    "dataset_path_val = os.path.join('Data', 'lucas')\n",
    "\n",
    "#cv_ = \"cv3\" # 5-fold cross-validation. Folds [cv1-cv5]\n",
    "\n",
    "# Transformations.\n",
    "transform_train = transforms.Compose([tio.transforms.RandomAffine(scales=(0.95, 1.05)),\n",
    "                                      #tio.transforms.RandomFlip(axes=[0, 1]),\n",
    "                                      tio.transforms.RandomMotion(), #tio.transforms.RandomBiasField(), \n",
    "                                      tio.transforms.RandomNoise()])\n",
    "\n",
    "#transform_train2 = tio.OneOf({tio.transforms.RandomAffine(scales=(0.95, 1.05)),\n",
    "#                                 tio.transforms.RandomFlip(axes=[0, 1]),\n",
    "#                                 tio.transforms.RandomBiasField(), tio.transforms.RandomMotion(), \n",
    "#                                 tio.RandomNoise()})\n",
    "\n",
    "transform_eval = None # transforms.Compose([Mask()])\n",
    "\n",
    "\n",
    "# Split dataset.\n",
    "train = MRIDataset(dataset_train, \"train\", dataset_path_train, transform=transform_train)\n",
    "train_queue = tio.Queue(subjects_dataset=train.dataset, max_length=216, samples_per_volume=8,\n",
    "                        sampler=tio.UniformSampler(patch_size=params[\"patch_size\"]), num_workers=4)\n",
    "\n",
    "val = MRIDataset(dataset_val, \"val\", dataset_path_val, transform=transform_eval)\n",
    "test = MRIDataset(dataset_val, \"test\", dataset_path_val, transform=transform_eval)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "train_loader = DataLoader(dataset=train_queue, batch_size=params[\"batch_size\"], num_workers=0, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val, batch_size=params[\"batch_size\"])\n",
    "test_loader = DataLoader(dataset=test, batch_size=params[\"batch_size\"])\n",
    "\n",
    "# Add dataset configuration to parameters to save them as meta data.\n",
    "params[\"dataset_train\"] = str(dataset_train).split(\"'\")[1].split('.')[-1]\n",
    "params[\"dataset_path_train\"] = dataset_path_train\n",
    "params[\"dataset_val\"] = str(dataset_train).split(\"'\")[1].split('.')[-1]\n",
    "params[\"dataset_path_val\"] = dataset_path_val\n",
    "#params[\"cross_validation\"] = \"None\" if not cv_ else cv_\n",
    "params[\"transform_train\"] = \"None\" if not transform_train else str(transform_train.transforms)\n",
    "params[\"transform_eval\"] = \"None\" if not transform_eval else str(transform_eval.transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = models.SegNet3D().to(device)\n",
    "model = models.SDUNet3D().to(device)\n",
    "criterion = DC_and_CE_loss({'batch_dice': True, 'smooth': 1e-5, 'do_bg': False, 'square': False}, {})\n",
    "#criterion = IoULoss()\n",
    "pretrained = False\n",
    "\n",
    "# Initalize weights or load already trained model.\n",
    "if not pretrained:\n",
    "#    model.apply(init_weights_kaiming)\n",
    "    params[\"initial_weights\"] = 'Random' #init_weights_kaiming.__name__\n",
    "else:\n",
    "    model_path = \"output/UNet3D/Iteration_20222812/run_dHCP/weights/36_model.pth\"\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    params[\"initial_weights\"] = model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHHDkEKkXYg_",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=params[\"SGD\"][\"lr\"], \n",
    "                            momentum=params[\"SGD\"][\"momentum\"], nesterov=params[\"SGD\"][\"nesterov\"])\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=params[\"CLR\"][\"base\"], \n",
    "                                              max_lr=params[\"CLR\"][\"max\"],\n",
    "                                              step_size_up=params[\"CLR\"][\"up\"], \n",
    "                                              step_size_down=params[\"CLR\"][\"down\"],\n",
    "                                              mode=params[\"CLR\"][\"mode\"])\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=params[\"SLR\"][\"step_size\"], \n",
    "                                            #gamma=params[\"SLR\"][\"gamma\"])\n",
    "    \n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, params[\"CALR\"][\"T_max\"], \n",
    "#                                                       eta_min=params[\"CALR\"][\"eta_min\"])\n",
    "\n",
    "early_stopping = EarlyStopping(patience=params[\"ES\"][\"patience\"], min_delta=params[\"ES\"][\"min_delta\"])\n",
    "\n",
    "\n",
    "# Initalize trainer for training.\n",
    "trainer = Trainer3D(criterion, model, optimizer, params[\"total_epochs\"], train_loader, scheduler)\n",
    "\n",
    "# Initalize evaluator for validation.\n",
    "evaluator = Evaluator3D(criterion, model, params[\"patch_size\"], val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output and path if it is not exist.\n",
    "if not os.path.isdir(weight_path):\n",
    "    os.makedirs(weight_path)\n",
    "\n",
    "# Create Tensorboard object to save experiment outputs.    \n",
    "tb = TensorboardModules(output_path)\n",
    "\n",
    "# Save hyperparameters as note.\n",
    "(pd.DataFrame.from_dict(data=params, orient='index')\n",
    " .to_csv(os.path.join(output_path,\"details.txt\"), header=False, sep=\"=\"))\n",
    "\n",
    "# Add some images and corresponding masks into Tensorboard.\n",
    "mri_image, mri_mask = val[0]['mri']['data'].squeeze(0), val[0]['mask']['data'].squeeze(0)\n",
    "slices = (50, 100, 10)\n",
    "tb.add_image_mask(mri_image, mri_mask, slices)\n",
    "\n",
    "# Add model graph to Tensorboard.\n",
    "tb.add_graph(model, params[\"patch_size\"], device)\n",
    "# print(summary(model, input_size=(1, 32, 128, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2tSNZTXXaCA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prev_weights = \"\"\n",
    "prev_val_loss = 100\n",
    "\n",
    "for epoch in range(0, params[\"total_epochs\"]):\n",
    "    # One forward pass for all training data.\n",
    "    avg_train_loss = trainer.fit()\n",
    "    \n",
    "    # Evaluate current model on validation data.\n",
    "    avg_val_loss, dice_scores = evaluator.evaluate()\n",
    "    avg_scores = sum(dice_scores) / len(dice_scores)\n",
    "    \n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    \n",
    "    # Add results to tensorboard.\n",
    "    tb.add_scalars(step=epoch+1, lr=scheduler.get_last_lr()[0], ds=avg_scores, \n",
    "                   train_loss=avg_train_loss, val_loss=avg_val_loss)\n",
    "    \n",
    "    model_name = \"_\".join([str(epoch), \"model.pth\"])\n",
    "    model_path = os.path.join(weight_path, model_name)\n",
    "    \n",
    "    if avg_val_loss < prev_val_loss:\n",
    "        # Save trained weights.\n",
    "        if os.path.isfile(prev_weights):\n",
    "            os.remove(prev_weights)        \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "    prev_weights = model_path        \n",
    "    prev_val_loss = avg_val_loss\n",
    "    \n",
    "    # If model is not learning stop the training.\n",
    "    early_stopping(avg_val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        break\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section: Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tissue_classes = [\"Background\", \"eCSF\", \"Gray Matter\", \"White Matter\", \"Ventricles\", \n",
    "                  \"Cerrebilium\", \"Deep Gray Matter\", \"Brain Stem\"]\n",
    "\n",
    "# Evaluate the last model on validation set.\n",
    "evaluator = Evaluator3D(criterion, model, params[\"patch_size\"], val_loader)\n",
    "val_loss, val_scores = evaluator.evaluate()\n",
    "avg_val_scores = sum(val_scores) / len(val_scores)\n",
    "# Convert Tensors to list.\n",
    "val_scores = [score.tolist() for score in val_scores]\n",
    "# Combine results and subject information to examine data carefully. \n",
    "val_results = pd.DataFrame(val_scores, index=val.meta_data[\"participant_id\"], columns=tissue_classes)\n",
    "val_results.drop(columns=\"Background\", inplace=True)\n",
    "val_results = pd.merge(val.meta_data, val_results, on=[\"participant_id\"])\n",
    "\n",
    "# Display results.\n",
    "print(f\"Average Validation Dice Scores{avg_val_scores}\")\n",
    "plt.boxplot(val_results.iloc[:, 3:]) # Plot only dice scoress in box plot.\n",
    "plt.show()\n",
    "val_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the last model on validation set. \n",
    "# If cross-validation was used test set is not available for evaluation.\n",
    "tissue_classes = [\"Background\", \"eCSF\", \"Gray Matter\", \"White Matter\", \"Ventricles\", \n",
    "                  \"Cerrebilium\", \"Deep Gray Matter\", \"Brain Stem\"]\n",
    "\n",
    "evaluator = Evaluator3D(criterion, model, params[\"patch_size\"], test_loader)\n",
    "test_loss, test_scores = evaluator.evaluate()\n",
    "avg_test_scores = sum(test_scores) / len(test_scores)\n",
    "# Convert Tensors to list.\n",
    "test_scores = [score.tolist() for score in test_scores]\n",
    "# Combine results and subject information to examine data carefully. \n",
    "test_results = pd.DataFrame(test_scores, index=test.meta_data[\"participant_id\"], columns=tissue_classes)\n",
    "test_results.drop(columns=\"Background\", inplace=True)\n",
    "test_results = pd.merge(test.meta_data, test_results, on=[\"participant_id\"])\n",
    "\n",
    "# Display results.\n",
    "print(f\"Average Test Dice Scores{avg_test_scores}\")\n",
    "plt.boxplot(test_results.iloc[:, 3:])\n",
    "plt.show()\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw an example output of trained model.\n",
    "mri, mask = val[6]\n",
    "pred = F.softmax(evaluator.predict(mri.view(1, *mri.shape)), dim=1)\n",
    "pred = torch.argmax(pred, dim=1)\n",
    "\n",
    "mask2 = mask.clone()\n",
    "index = 65\n",
    "class_id = 0\n",
    "mask2[:, index, :][mask2[:, index, :]!=class_id] = 0\n",
    "plot_sub(mri[:, :, index], mask2[:, :, index], pred[0, :, :, index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outputs.\n",
    "output_folder = \"eval\"\n",
    "\n",
    "for i, (mri, mask) in enumerate(val_loader):\n",
    "    pred = F.softmax(evaluator.predict(mri), dim=1)\n",
    "    one_hot_mask = create_onehot_mask(pred.shape, mask.unsqueeze(0))\n",
    "    pred = torch.argmax(pred, dim=1, keepdim=True)\n",
    "    one_hot_pred = create_onehot_mask(one_hot_mask.shape, pred)\n",
    "    dice_scores = calculate_dice_score(one_hot_pred, one_hot_mask)\n",
    "    with open(os.path.join(output_folder, f\"{i+1}.txt\"), 'w') as writer:\n",
    "        writer.write(str([round(score, 3) for score in dice_scores.tolist()]))\n",
    "        \n",
    "    pred = pred.squeeze()\n",
    "    mri = mri.squeeze().detach().cpu().numpy()\n",
    "    mask = mask.squeeze().detach().cpu().numpy()\n",
    "    pred = pred.squeeze().detach().cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    mri_name = f\"{i+1}_mri\"\n",
    "    mask_name = f\"{i+1}_mask\"\n",
    "    pred_name = f\"{i+1}_prediction\"\n",
    "    \n",
    "    save_nii(output_folder, mri_name, mri)\n",
    "    save_nii(output_folder, mask_name, mask)\n",
    "    save_nii(output_folder, pred_name, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "UNet_3D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
